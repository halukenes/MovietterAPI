{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>isRT</th>\n",
       "      <th>time</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1196130948628152321</td>\n",
       "      <td>False</td>\n",
       "      <td>17.11.2019 18:20:15</td>\n",
       "      <td>en</td>\n",
       "      <td>@bleedingcool Man of steel?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1195991070393257984</td>\n",
       "      <td>False</td>\n",
       "      <td>17.11.2019 09:04:26</td>\n",
       "      <td>en</td>\n",
       "      <td>@IMDb Doctor strange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1195478171111444481</td>\n",
       "      <td>False</td>\n",
       "      <td>15.11.2019 23:06:21</td>\n",
       "      <td>en</td>\n",
       "      <td>@TwitterMovies What are both of yours next pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1195252904447238145</td>\n",
       "      <td>False</td>\n",
       "      <td>15.11.2019 08:11:13</td>\n",
       "      <td>en</td>\n",
       "      <td>@TheEllenShow @SteveSpangler @andylassner He s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1195252811535015936</td>\n",
       "      <td>False</td>\n",
       "      <td>15.11.2019 08:10:51</td>\n",
       "      <td>en</td>\n",
       "      <td>@andylassner @TheEllenShow @SteveSpangler You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1209241280263229440</td>\n",
       "      <td>False</td>\n",
       "      <td>23.12.2019 22:36:02</td>\n",
       "      <td>en</td>\n",
       "      <td>@Fandango Escape room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1209108460660412416</td>\n",
       "      <td>False</td>\n",
       "      <td>23.12.2019 13:48:15</td>\n",
       "      <td>en</td>\n",
       "      <td>@TysMae @randymoncesart @johncampea That's nyc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1209076525347131392</td>\n",
       "      <td>False</td>\n",
       "      <td>23.12.2019 11:41:21</td>\n",
       "      <td>en</td>\n",
       "      <td>@randymoncesart @johncampea So is it fun for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1208821162903400448</td>\n",
       "      <td>False</td>\n",
       "      <td>22.12.2019 18:46:38</td>\n",
       "      <td>en</td>\n",
       "      <td>@johncampea So can it be new got???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1208491000689123329</td>\n",
       "      <td>False</td>\n",
       "      <td>21.12.2019 20:54:41</td>\n",
       "      <td>en</td>\n",
       "      <td>You would love that experience .@PNemiroff won...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id   isRT                 time lang  \\\n",
       "0   1196130948628152321  False  17.11.2019 18:20:15   en   \n",
       "1   1195991070393257984  False  17.11.2019 09:04:26   en   \n",
       "4   1195478171111444481  False  15.11.2019 23:06:21   en   \n",
       "5   1195252904447238145  False  15.11.2019 08:11:13   en   \n",
       "6   1195252811535015936  False  15.11.2019 08:10:51   en   \n",
       "..                  ...    ...                  ...  ...   \n",
       "82  1209241280263229440  False  23.12.2019 22:36:02   en   \n",
       "83  1209108460660412416  False  23.12.2019 13:48:15   en   \n",
       "84  1209076525347131392  False  23.12.2019 11:41:21   en   \n",
       "85  1208821162903400448  False  22.12.2019 18:46:38   en   \n",
       "87  1208491000689123329  False  21.12.2019 20:54:41   en   \n",
       "\n",
       "                                                 text  \n",
       "0                         @bleedingcool Man of steel?  \n",
       "1                                @IMDb Doctor strange  \n",
       "4   @TwitterMovies What are both of yours next pro...  \n",
       "5   @TheEllenShow @SteveSpangler @andylassner He s...  \n",
       "6   @andylassner @TheEllenShow @SteveSpangler You ...  \n",
       "..                                                ...  \n",
       "82                              @Fandango Escape room  \n",
       "83  @TysMae @randymoncesart @johncampea That's nyc...  \n",
       "84  @randymoncesart @johncampea So is it fun for p...  \n",
       "85                @johncampea So can it be new got???  \n",
       "87  You would love that experience .@PNemiroff won...  \n",
       "\n",
       "[65 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"./twitterdata/\"\n",
    "\n",
    "user_timeline = pd.read_csv(folder_path + 'Hwoodmoviegeek_timeline.csv', encoding='utf-8')\n",
    "\n",
    "filtered_timeline = user_timeline[(user_timeline.isRT == False) & (user_timeline.lang == 'en')]\n",
    "\n",
    "filtered_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haluk\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([' man of steel ', ' doctor strange',\n",
       "       ' what are both of yours next projects ',\n",
       "       ' he should get a raise man', ' you should get a raise dude',\n",
       "       ' i think   nailed it    it was the best episode',\n",
       "       ' did anyone masturbate at this   ',\n",
       "       ' i totally love animated movies too',\n",
       "       ' the updated opening weekend was 96 million dollars',\n",
       "       ' once my mom wanted to have dinner with me  i said i have a date tonight  she said   yeah right ',\n",
       "       ' inception hands down',\n",
       "       ' man of steel  infinity war  the dark knight rises',\n",
       "       ' breaking bad finale',\n",
       "       ' yeah but i think it is the highest without chinese release',\n",
       "       ' it was fine', ' stop my parents from having me ', ' prisoners',\n",
       "       ' the avengers', ' inception',\n",
       "       ' harry potter and the deathly hallows part 2',\n",
       "       ' obviously inception', ' inception', ' inception period',\n",
       "       'ohh  would love if that would happen wouldn t he   ',\n",
       "       ' chewbaccha', ' escape room     because i lvoe that genre',\n",
       "       'dude i cannot sleep waiting for this movie everyday day    i am dying to watch this ',\n",
       "       ' michael scott    rachel    joey    phil dunphy',\n",
       "       ' do you think the second will be better than 1st ', ' inception',\n",
       "       ' i don t deserve you', ' unusually crazy', ' hostel',\n",
       "       ' playmobil   ', ' inception', ' the music is fantaaaaaaastic',\n",
       "       ' the music is fantaaaaaaasticc', ' my best movie of all time too',\n",
       "       ' escape room', ' your reaction is just that  ', ' money heist',\n",
       "       ' the lion king should win    whether you like it or not   those visual effects were beyond stunning',\n",
       "       ' tenet dude', ' 8 of them are disney',\n",
       "       ' i loved rogue one more than any star wars movies    even the original ones',\n",
       "       ' hell they will have 7 bd films in a single year    who can beat that',\n",
       "       ' pure class', ' brilliant movie', ' inception',\n",
       "       ' ohh okk     what about scar jo then',\n",
       "       ' that s what i said    including domestic grosses for 2 films he have been in i  ',\n",
       "       ' i hope the present is good    otherwise he will hate marvel even more ',\n",
       "       ' inception',\n",
       "       ' so all time is more than that isn t it   they have been in 4 avengers movies',\n",
       "       ' john wick   myles morales   harley quinn    james bond and dominic toretto',\n",
       "       ' i don t understand    chris evans had only endgame and knives out   they made a billion dollars combined    how is it 1 3 ',\n",
       "       ' i really respect people s opinions    but for me    this was the worst ranking i have ever  ',\n",
       "       ' but i m sure people will want to see new adaptations of classics if they are done  ',\n",
       "       ' although i can understand what you are saying    but this that they had will not be achieved by any  ',\n",
       "       ' and look how that worked out for them    dude they know what people want to see and did it    wheth  ',\n",
       "       ' escape room', ' that s nyc   i m excited now',\n",
       "       ' so is it fun for people who didn t read any books or played any video games ',\n",
       "       ' so can it be new got   ',\n",
       "       'you would love that experience  won t you    '], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    " \n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: x.lower())\n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: re.sub(r'(^|[^@\\w])@(\\w{1,15})\\b', '', x)) #user tags\n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: re.sub(r'(^|[^@\\w])#(\\w{1,15})\\b', '', x)) #user tags\n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: re.sub(r'http\\S+',  '', x))   #urls \n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: x.replace('\\n','')) \n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "filtered_timeline.loc[:,'text'] = filtered_timeline['text'].apply(deEmojify) #emojis\n",
    "filtered_timeline = filtered_timeline.drop(filtered_timeline[filtered_timeline.text == ''].index) #delete empty strings\n",
    "filtered_timeline['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 5.551051616668701 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['man steel',\n",
       " 'doctor strange',\n",
       " 'next project',\n",
       " 'get raise man',\n",
       " 'get raise dude',\n",
       " 'think nail best episode',\n",
       " 'anyone masturbate',\n",
       " 'totally love animate movie',\n",
       " 'update opening weekend 96 million dollar',\n",
       " 'mom want dinner say date tonight say yeah right',\n",
       " 'inception hand',\n",
       " 'man steel infinity war dark knight rise',\n",
       " 'break bad finale',\n",
       " 'yeah think high without chinese release',\n",
       " 'fine',\n",
       " 'stop parent',\n",
       " 'prisoner',\n",
       " 'avenger',\n",
       " 'inception',\n",
       " 'harry potter deathly hallows part 2',\n",
       " 'obviously inception',\n",
       " 'inception',\n",
       " 'inception period',\n",
       " 'ohh would love would happen',\n",
       " 'chewbaccha',\n",
       " 'escape room lvoe genre',\n",
       " 'dude sleep wait movie everyday day die watch',\n",
       " 'michael scott rachel joey phil dunphy',\n",
       " 'think second well 1st',\n",
       " 'inception',\n",
       " 'deserve',\n",
       " 'unusually crazy',\n",
       " 'hostel',\n",
       " 'playmobil',\n",
       " 'inception',\n",
       " 'music fantaaaaaaastic',\n",
       " 'music fantaaaaaaasticc',\n",
       " 'best movie time',\n",
       " 'escape room',\n",
       " 'reaction',\n",
       " 'money heist',\n",
       " 'lion king win whether like visual effect beyond stun',\n",
       " 'tenet dude',\n",
       " '8 disney',\n",
       " 'love rogue one star war movie even original one',\n",
       " 'hell 7 bd film single year beat',\n",
       " 'pure class',\n",
       " 'brilliant movie',\n",
       " 'inception',\n",
       " 'ohh okk scar jo',\n",
       " 'say include domestic gross 2 film',\n",
       " 'hope present good otherwise hate marvel even',\n",
       " 'inception',\n",
       " 'time 4 avenger movie',\n",
       " 'john wick myles morale harley quinn james bond dominic toretto',\n",
       " 'understand chris evans endgame knife make billion dollar combine 1 3',\n",
       " 'really respect people opinion bad rank ever',\n",
       " 'sure people want see new adaptation classic do',\n",
       " 'although understand say achieve',\n",
       " 'look work dude know people want see wheth',\n",
       " 'escape room',\n",
       " 'nyc excite',\n",
       " 'fun people read book played video game',\n",
       " 'new get',\n",
       " 'would love experience']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from pywsd.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('didnt')\n",
    "stop_words.add('dont')\n",
    "stop_words.add('youre')\n",
    "stop_words.add('im')\n",
    "\n",
    "def tokenization_w(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        w_token = word_tokenize(text)\n",
    "        filtered_sentence = [w for w in w_token]\n",
    "        tokenized_texts.append(filtered_sentence)\n",
    "    return tokenized_texts\n",
    "\n",
    "def lemmatization(stem_array):\n",
    "    lemmatized = []\n",
    "    for stems in stem_array:\n",
    "        lemmas = [lemmatize(x) for x in stems if not x in stop_words]\n",
    "        lemmatized.append(lemmas)\n",
    "    return lemmatized\n",
    "\n",
    "tokens = tokenization_w(filtered_timeline['text'])\n",
    "lemmatized_data = lemmatization(tokens)\n",
    "data = [' '.join(list) for list in lemmatized_data]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'steel']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(bigram_mod[data_words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['strange'],\n",
       " ['next', 'project'],\n",
       " ['raise', 'man'],\n",
       " ['raise', 'dude'],\n",
       " ['think', 'good', 'episode'],\n",
       " [],\n",
       " ['totally', 'love', 'animate', 'movie'],\n",
       " ['update', 'opening', 'weekend', 'dollar'],\n",
       " ['mom', 'want', 'dinner', 'say', 'date', 'tonight', 'say'],\n",
       " [],\n",
       " [],\n",
       " ['break', 'bad', 'finale'],\n",
       " ['think', 'high', 'chinese', 'release'],\n",
       " ['fine'],\n",
       " ['stop', 'parent'],\n",
       " ['prisoner'],\n",
       " [],\n",
       " ['inception'],\n",
       " ['hallow', 'part'],\n",
       " ['obviously', 'inception'],\n",
       " ['inception'],\n",
       " ['inception', 'period'],\n",
       " ['would', 'love', 'would', 'happen'],\n",
       " [],\n",
       " ['room'],\n",
       " ['wait', 'movie', 'everyday', 'day', 'die', 'watch'],\n",
       " [],\n",
       " ['think', 'second', 'well'],\n",
       " ['inception'],\n",
       " ['deserve'],\n",
       " ['unusually', 'crazy'],\n",
       " [],\n",
       " [],\n",
       " ['inception'],\n",
       " ['music'],\n",
       " ['music'],\n",
       " ['good', 'movie', 'time'],\n",
       " ['room'],\n",
       " ['reaction'],\n",
       " ['money'],\n",
       " ['win', 'visual', 'effect', 'stun'],\n",
       " ['dude'],\n",
       " [],\n",
       " ['love', 'war', 'movie', 'even', 'original'],\n",
       " ['film', 'single', 'year', 'beat'],\n",
       " ['pure', 'class'],\n",
       " ['brilliant'],\n",
       " ['inception'],\n",
       " ['scar'],\n",
       " ['say', 'include', 'domestic', 'gross', 'film'],\n",
       " ['hope', 'present', 'good', 'otherwise', 'hate', 'marvel', 'even'],\n",
       " ['inception'],\n",
       " ['movie'],\n",
       " ['myle'],\n",
       " ['make', 'dollar', 'combine'],\n",
       " ['really', 'respect', 'people', 'opinion', 'bad', 'rank', 'ever'],\n",
       " ['sure', 'people', 'want', 'see', 'new', 'adaptation', 'classic'],\n",
       " ['understand', 'say', 'achieve'],\n",
       " ['look', 'work', 'dude', 'know', 'people', 'want', 'see'],\n",
       " ['room'],\n",
       " [],\n",
       " ['fun', 'people', 'read', 'play', 'video', 'game'],\n",
       " ['get'],\n",
       " ['would', 'love', 'experience']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.056*\"say\" + 0.039*\"even\" + 0.021*\"dinner\" + 0.021*\"present\" + '\n",
      "  '0.021*\"marvel\" + 0.021*\"mom\" + 0.021*\"otherwise\" + 0.021*\"video\" + '\n",
      "  '0.021*\"hope\" + 0.021*\"date\"'),\n",
      " (1,\n",
      "  '0.154*\"inception\" + 0.023*\"movie\" + 0.023*\"raise\" + 0.023*\"film\" + '\n",
      "  '0.023*\"think\" + 0.023*\"everyday\" + 0.023*\"watch\" + 0.023*\"die\" + '\n",
      "  '0.023*\"wait\" + 0.023*\"year\"'),\n",
      " (2,\n",
      "  '0.068*\"room\" + 0.047*\"people\" + 0.026*\"bad\" + 0.026*\"love\" + 0.026*\"want\" + '\n",
      "  '0.026*\"opinion\" + 0.026*\"rank\" + 0.026*\"respect\" + 0.026*\"really\" + '\n",
      "  '0.026*\"classic\"'),\n",
      " (3,\n",
      "  '0.053*\"good\" + 0.053*\"think\" + 0.053*\"movie\" + 0.029*\"episode\" + '\n",
      "  '0.029*\"break\" + 0.029*\"love\" + 0.029*\"well\" + 0.029*\"finale\" + '\n",
      "  '0.029*\"experience\" + 0.029*\"second\"'),\n",
      " (4,\n",
      "  '0.045*\"would\" + 0.045*\"dude\" + 0.045*\"music\" + 0.025*\"love\" + 0.025*\"want\" '\n",
      "  '+ 0.025*\"dollar\" + 0.025*\"see\" + 0.025*\"people\" + 0.025*\"look\" + '\n",
      "  '0.025*\"work\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'im',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['say',\n",
       "  'even',\n",
       "  'dinner',\n",
       "  'present',\n",
       "  'marvel',\n",
       "  'mom',\n",
       "  'otherwise',\n",
       "  'video',\n",
       "  'hope',\n",
       "  'date'],\n",
       " ['inception',\n",
       "  'movie',\n",
       "  'raise',\n",
       "  'film',\n",
       "  'think',\n",
       "  'everyday',\n",
       "  'watch',\n",
       "  'die',\n",
       "  'wait',\n",
       "  'year'],\n",
       " ['room',\n",
       "  'people',\n",
       "  'bad',\n",
       "  'love',\n",
       "  'want',\n",
       "  'opinion',\n",
       "  'rank',\n",
       "  'respect',\n",
       "  'really',\n",
       "  'classic'],\n",
       " ['good',\n",
       "  'think',\n",
       "  'movie',\n",
       "  'episode',\n",
       "  'break',\n",
       "  'love',\n",
       "  'well',\n",
       "  'finale',\n",
       "  'experience',\n",
       "  'second'],\n",
       " ['would',\n",
       "  'dude',\n",
       "  'music',\n",
       "  'love',\n",
       "  'want',\n",
       "  'dollar',\n",
       "  'see',\n",
       "  'people',\n",
       "  'look',\n",
       "  'work']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=lda_model.show_topics(num_topics=8, num_words=10,formatted=False)\n",
    "topics_words = [[wd[0] for wd in tp[1]] for tp in x]\n",
    "\n",
    "topics = []\n",
    "for words in topics_words:\n",
    "    topics.append(words)\n",
    "    \n",
    "topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
